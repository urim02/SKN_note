{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence (seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 모델의 한계: 입력과 출력이 고정된 차원의 벡터로 인코딩될 수 있는 문제에만 적용 가능<br>\n",
    "-> 음성 인식과 기계 번역 및 질문 응답 등은 sequence문제이며, sequence를 sequence로 mapping할 수 있는 도메인 독립적 방법 요구됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            latent_dim,\n",
    "            vocab_size,\n",
    "            n_layers: int = 4,\n",
    "            teacher_forcing_ratio: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim                                  # 입력 특징 차원\n",
    "        self.latent_dim = latent_dim                                # 잠재 공간(은닉 상태)의 차원        \n",
    "        self.vocab_size = vocab_size                                # 어휘 크기(출력 차원)\n",
    "        self.n_layers = n_layers                                    # 인코더의 레이어 수\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio          # 교사 강요 사용 확률\n",
    "\n",
    "        # 인코더: 입력 시퀀스를 처리하는 LSTM 네트워크\n",
    "        self.encoder = nn.LSTM(\n",
    "            self.input_dim, \n",
    "            self.latent_dim,\n",
    "            num_layers=self.n_layers,\n",
    "            batch_first=True,                       # 입력과 출력 텐서가 (배치, 시퀀스, 특징) 형태로 제공됨\n",
    "        )\n",
    "\n",
    "        # 디코더: 출력 시퀀스를 생성하는 일련의 LSTM 레이어\n",
    "        self.decoder1 = nn.LSTM(                    \n",
    "            self.latent_dim, \n",
    "            128,\n",
    "            batch_first=True,\n",
    "            num_layers=4\n",
    "        )\n",
    "        self.decoder2 = nn.LSTM(\n",
    "            128,\n",
    "            128,\n",
    "            batch_first=True,\n",
    "            )\n",
    "        self.decoder3 = nn.LSTM(\n",
    "            128,\n",
    "            128,\n",
    "            batch_first=True,\n",
    "            )\n",
    "        self.decoder4 = nn.LSTM(\n",
    "            128,\n",
    "            1000,                                   # 최종 레이어의 출력 차원\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # 소스 시퀀스를 인코딩하여 잠재 벡터를 얻음\n",
    "        output, latent_vector = self.encoder(source)\n",
    "        # 디코더의 초기 입력을 0으로 초기화\n",
    "        x = torch.zeros_like(output)[:, :1, :]\n",
    "        # 첫 번째 디코더 레이어를 통과\n",
    "        output, (h_n, c_n) = self.decoder1(x, latent_vector)\n",
    "        # 첫 번째 예측 토큰을 얻음\n",
    "        next_token = output[:, 0, :].argmax(axis = -1)\n",
    "        outputs = [next_token]                      # 모든 예측 토큰을 저장할 리스트\n",
    "\n",
    "        for t in range(1, target.shape[1]):\n",
    "            # 교사 강요 사요이 실제 타겟을 다음 입력으로 사용\n",
    "            if np.random.random() > self.teacher_forcing_ratio:\n",
    "                output = target[:, t, :]\n",
    "            # 후속 디코더 레이어를 통과\n",
    "            output, (h_n, c_n) = self.decoder(output, (h_n, c_n))\n",
    "\n",
    "            next_token = output[:, 0, :].argmax(axis = -1)\n",
    "            outputs.append(next_token)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder class**\n",
    "\n",
    "1. Embedding layer:\n",
    "- 입력 시퀀스의 각 단어를 고정된 크기의 벡터로 변환한다.<br>이는 단어를 수치적으로 표현하여 모델이 처리할 수 있도록 한다.<br>\n",
    "- 입력 데이터를 LSTM 레이어에 전달하기 전에 필수적인 전처리 단계\n",
    "\n",
    "2. LSTM layer:\n",
    "- 시퀀스 데이터를 처리하여 각 타임스텝의 출력과 마지막 hidden state 및 cell state를 계산<br>\n",
    "- LSTM은 시퀀스 데이터의 시간적 의존성을 학습하며, 인코더의 핵심 역할을 담당.<br>여러 레이어로 구성되어 더 복잡한 패턴을 학습 가능\n",
    "\n",
    "3. Latent vector 생성:\n",
    "- 마지막 hidden state와 cell state를 결합하여 latent vector 생성.<br>이는 인코더가 입력 시퀀스를 요약한 정보.\n",
    "- latent vector는 디코더로 전달되어 디코더가 출력을 생성하는 데 필요한 컨텍스트 정보를 제공. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, latent_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size                # 입력 시퀀스의 단어 집합 크기. Embedding layer에서 각 단어를 고유한 벡터로 변환하기 위해 필요\n",
    "        self.embedding_dim = embedding_dim              # 단어 embedding의 찬원. 각 단어를 고정된 크기의 벡터로 표현하는 데 사용.\n",
    "        self.latent_dim = latent_dim                # LSTM의 hidden_state, cell_state의 차원\n",
    "        self.num_layers = num_layers                # LSTM layer의 수\n",
    "\n",
    "        # 단어를 벡터로 변환하는 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # 입력 처리할 LSTM 레이어 정의\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.embedding_dim,\n",
    "            self.latent_dim,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = True,\n",
    "        )\n",
    "\n",
    "        def forward(self, x):\n",
    "            # 입력 시퀀스를 임베딩 벡터로 변환\n",
    "            x = self.embedding(x)\n",
    "            # LSTM를 통하여 각 time step 의 출력화 마지막 hidden state와 cell state 계산\n",
    "            x, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "            # 마지막 hidden state와 cell state를 결합하여 latent vector 생성\n",
    "            latent_vector = torch.car([h_n[-1], c_n[-1]], axis = -1)\n",
    "            latent_vector = latent_vector[:, np.newaxis, :] # 차원 추가\n",
    "\n",
    "            return latent_vector\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size                # 출력 시퀀스의 단어 집합 크기.최종\n",
    "        self.embedding_dim = embedding_dim          # 입력 단어를 벡터로 변환하는 데 사용됨. 인코더와 동일한 차원을 사용\n",
    "        self.latent_dim = latent_dim                # LSTM의 hidden state, cell state의 차원. Encoder와 동일한 차원을 사용하여 latent vector와 결합\n",
    "\n",
    "        # 단어를 벡터로 변환하는 embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # 여러 개의 LSTM layer 정의하여 입력을 처리\n",
    "        self.lstm1 = nn.LSTM(self.embedding_dim + self.latent_dim*2, self.latent_dim, batch_size = True)               \n",
    "        # self.embedding_dim: 현재 입력 단어의 embedding 벡터 차원\n",
    "        # self.latent_dim*2: encoder에서 생성된 latent vector(마지막 hidden state, cell state결합)의 차원\n",
    "        self.lstm2 = nn.LSTM(self.latent_dim, self.latent_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(self.latent_dim, self.latent_dim, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(self.latent_dim, self.latent_dim, batch_first=True)\n",
    "        # 최종 출력 레이어로 단어 확률 분호 생성\n",
    "        self.fc_out = nn.Linear(self.latent_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state, latent_vector):\n",
    "        x = x[:, np.newaxis]                        # 입력 차원 추가 (배치 크기, 1)\n",
    "        # 입력을 embedding vector로 변환\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # embedding과 latent vector를 결합하여 LSTM에 전달할 입력 생성\n",
    "        x = torch.cat([x, latent_vector], axis=-1)\n",
    "\n",
    "        # 순차적으로 여러 LSTM layer를 통과하며 출력 계산\n",
    "        x, _ = self.lstm1(x, (hidden_state, cell_state))\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "\n",
    "        # 마지막 LSTM layer의 출력과 상태 반환\n",
    "        x, (h_n, c_n)= self.lstm4(x)\n",
    "        # 최종 출력 생성 (단어 확률 분호)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x, (h_n, c_n)                        # 출력 및 마지막 hidden state와 cell state 반환\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio        \n",
    "    \n",
    "    def forward(self, source, target):\n",
    "        batch_size = len(source)                    \n",
    "        target_length = target.shaper[1]\n",
    "        target_vocab_size = self.decoder.vocab_size         # 디코더의 어휘 크기\n",
    "        # 출력을 저장할 텐서 초기화 (배치 크기 x 목표 시퀀스 길이 x 어휘 크기)\n",
    "        outputs = torch.zeros(batch_size, target_length, target_vocab_size)\n",
    "\n",
    "        # encoder를 통해 latent vector 생성\n",
    "        latent_vector = self.encoder(source)\n",
    "        # 첫 번째 decoder 입력은 <SOS> token 사용\n",
    "        x = target[:, 0]\n",
    "        h_n = torch.zeros(1, batch_size, self.encoder.latent_dim)           # 초기 hidden state 설정\n",
    "        c_n = torch.zeros(1, batch_size, self.encoder.latent_dim)           # 초기 cell state 설정\n",
    "\n",
    "        # 목표 시퀀스의 각 타입스텝에 대한 반복\n",
    "        for t in range(1, target_length):\n",
    "            output, (h_n, c_n) = self.decoder(x, h_n, c_n, latent_vector)   # decoder를 통해 출력 및 다음 hidden state, cell state 얻기\n",
    "            outputs[:, t-1, :] = output[:, 0, :]                            # 현재 타입스텝의 출력 저장\n",
    "\n",
    "            if np.random.random() < self.teacher_forcing_ratio:\n",
    "                x = output[:, 0, :].argmax(axis=-1)                         # 모델의 출력을 다음 입력으로 사용\n",
    "            else: \n",
    "                x = target[:, t]                                            # 실제 타겟을 다음 입력으로 사용\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터 생성\n",
    "source = torch.randint(0, 1000, (32, 20))   # 0 ~ 999의 정수 값을 갖는 (batch_size, seq_len) 행렬 생성\n",
    "target = torch.randint(0, 1000, (32, 20))   # 0 ~ 999의 정수 값을 갖는 (batch_size, seq_len) 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [Encoder] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# latent vector 생성\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m latent_vector \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:352\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [Encoder] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "# 인코더 초기화\n",
    "encoder = Encoder(1000, 256, 256, 4)\n",
    "\n",
    "# latent vector 생성\n",
    "latent_vector = Encoder(1000, 256, 256, 4)(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RNNBase.__init__() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m cell \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m256\u001b[39m)      \u001b[38;5;66;03m# (num_layers, batch_size, latent_dim)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 디코더 초기화\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m decoder \u001b[38;5;241m=\u001b[39m \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 디코더의 첫 번째 입력으로 target의 첫 번째 요소 전달\u001b[39;00m\n\u001b[0;32m      8\u001b[0m decoder(target[:, \u001b[38;5;241m0\u001b[39m], hidden, cell, latent_vector)\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[1;34m(self, vocab_size, embedding_dim, latent_dim)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 여러 개의 LSTM layer 정의하여 입력을 처리\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m               \n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# self.embedding_dim: 현재 입력 단어의 embedding 벡터 차원\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# self.latent_dim*2: encoder에서 생성된 latent vector(마지막 hidden state, cell state결합)의 차원\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:808\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: RNNBase.__init__() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "hidden = torch.zeros(1, 32, 256)    # (num_layers, batch_size, latent_dim)\n",
    "cell = torch.zeros(1, 32, 256)      # (num_layers, batch_size, latent_dim)\n",
    "\n",
    "# 디코더 초기화\n",
    "decoder = Decoder(1000, 256, 256)\n",
    "\n",
    "# 디코더의 첫 번째 입력으로 target의 첫 번째 요소 전달\n",
    "decoder(target[:, 0], hidden, cell, latent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# seq2seq 초기화\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m seq2seq \u001b[38;5;241m=\u001b[39m Seq2Seq(encoder, \u001b[43mdecoder\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "# seq2seq 초기화\n",
    "seq2seq = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq2seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# cross entropy 사용 방법에 맞게 축 변환\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mseq2seq\u001b[49m(source, target)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (batch_size, seq_len, dim) -> (batch_size, dim, seq_len)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# cross entropy loss 계산\u001b[39;00m\n\u001b[0;32m      5\u001b[0m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred, target)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seq2seq' is not defined"
     ]
    }
   ],
   "source": [
    "# cross entropy 사용 방법에 맞게 축 변환\n",
    "pred = seq2seq(source, target).permute(0, 2, 1) # (batch_size, seq_len, dim) -> (batch_size, dim, seq_len)\n",
    "\n",
    "# cross entropy loss 계산\n",
    "F.cross_entropy(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
